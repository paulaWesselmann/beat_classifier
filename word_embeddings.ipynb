{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings       (∩^o^)⊃━☆ﾟ.*･｡ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [What are word embeddings?](#what)\n",
    "2. [How are they trained?](#how)\n",
    "3. [How did we use them for Beat classification?](#beat)\n",
    "4. [Resources](#more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What are word embeddings? <a id='what'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- representation of a word in vector form\n",
    "- carry meaning and context of a word\n",
    "- similar words are closer together in the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pretrained spacy word embeddings \n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "size of vector:  300\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# word without meaning\n",
    "string_1 = 'drigakulope'\n",
    "embedding_1 = nlp('drigakulope')\n",
    "\n",
    "print(embedding_1[0].is_oov) # oov: out of vocabulary\n",
    "print('size of vector: ',embedding_1.vector.size)\n",
    "print(embedding_1.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[-3.2881e-01  2.1108e-01  4.3552e-02  1.3979e-01 -5.2884e-01 -5.1644e-02\n",
      " -3.3082e-01 -1.2381e-01 -2.7482e-02  3.2725e+00 -9.5697e-01 -1.8551e-01\n",
      " -1.3150e-01  1.9451e-01 -1.7047e-01  2.6562e-01  2.2098e-01  1.7486e+00\n",
      " -6.4204e-01 -7.7755e-02 -2.4530e-02  6.3740e-01 -3.0614e-02 -6.2041e-01\n",
      "  2.3933e-01 -3.6727e-01  6.0124e-02  2.3843e-01  2.6267e-01 -3.7098e-01\n",
      " -6.0564e-01 -6.9927e-02  3.3869e-01 -4.2650e-01  3.4224e-01  1.3294e-02\n",
      "  1.0972e-01  6.0027e-02 -4.1023e-01  4.4138e-01  2.9172e-01  3.3538e-01\n",
      " -5.1540e-01 -2.9832e-01 -2.3043e-01  1.5725e-01 -4.3485e-01  2.1103e-01\n",
      "  2.1040e-01 -8.0232e-02 -7.9495e-01 -2.6638e-01  6.9827e-01 -3.8734e-01\n",
      " -8.7617e-02  9.2663e-02  2.2422e-01  6.1503e-01  1.1925e-01 -5.3161e-01\n",
      " -4.1816e-02 -3.0765e-01 -1.6384e-01 -1.2057e-01  5.3617e-01  2.0648e-01\n",
      "  3.2788e-01 -1.9545e-02 -2.7603e-01  3.0034e-01 -6.0207e-01 -2.6588e-01\n",
      "  2.0489e-01 -1.0422e-01  7.3177e-01  2.5777e-01  1.0356e-01 -2.9657e-01\n",
      "  1.5593e-01 -1.1592e-01  3.4370e-02  1.9574e-01 -2.1414e-01  3.8057e-01\n",
      " -2.5573e-01 -3.9610e-01 -3.3810e-01  8.5079e-02 -2.4120e-01  1.0286e-01\n",
      "  3.6657e-02 -1.1592e-01 -1.6387e-01 -3.0408e-01 -4.7404e-01 -4.9560e-01\n",
      "  6.6344e-02  2.1715e-02 -9.6796e-02 -5.0327e-02 -1.9369e-01 -4.1357e-02\n",
      "  3.1388e-01 -1.3101e-01 -3.5704e-01 -1.6251e+00  1.0732e-01 -6.2876e-02\n",
      " -5.1270e-01 -2.0636e-01 -5.1127e-03  3.8329e-01 -5.9936e-01 -5.1844e-01\n",
      "  2.0495e-01 -2.6191e-02 -8.8076e-03 -6.2742e-03  3.1784e-01 -1.0431e-02\n",
      "  3.2659e-01  5.6179e-01  1.9495e-02 -7.3675e-01  6.2946e-02  1.6263e-01\n",
      "  3.6373e-01  4.8501e-02  1.5951e-01 -3.1200e-01  6.0345e-02 -3.9996e-01\n",
      "  2.3356e-01 -3.4421e-01  2.7090e-01  2.1531e-01 -4.9184e-02 -3.5621e-01\n",
      " -5.7325e-02  7.3295e-03  2.2404e-01  2.7215e-01  6.7054e-01  2.9539e-02\n",
      "  2.2605e-01 -2.1835e-01  2.1361e-01  2.4876e-01 -4.1475e-01 -7.9038e-02\n",
      " -3.2564e-01 -4.3497e-01  4.1842e-01  1.8752e-01  1.0096e-02 -8.1930e-02\n",
      "  4.7166e-01  1.1185e-01 -4.2660e-01 -1.3805e-02  5.0503e-01  5.2691e-01\n",
      "  1.3217e-01 -4.3790e-01  5.9767e-02  1.3710e-01  6.4892e-01 -3.1629e-01\n",
      " -1.4315e-01 -5.5207e-01  1.7710e-01 -2.8379e-01  4.0700e-01  7.4761e-02\n",
      " -1.3422e-01  2.3964e-01  1.7042e-01  7.1176e-01  7.1834e-01  6.3868e-01\n",
      " -3.2481e-01  1.6447e-02 -1.4155e-01  3.1020e-01  3.7969e-01 -1.7846e-01\n",
      " -1.5653e-01  9.5415e-02 -7.3072e-01  1.4173e-01 -3.3896e-01 -4.8991e-02\n",
      "  2.4626e-01 -5.1500e-01 -1.6697e-01  4.6497e-01  1.6621e-02 -3.3207e-03\n",
      "  5.7353e-01  2.4674e-01  9.7843e-04 -2.9414e-01 -1.5266e-01  1.4994e-01\n",
      " -4.7668e-01  5.6139e-01 -2.0623e-01  1.8259e-01 -1.2693e-01 -4.2322e-01\n",
      " -5.1254e-01 -4.5170e-02 -2.4755e-01  2.1950e-01 -2.7637e-01 -8.6335e-01\n",
      "  5.1941e-01  2.9018e-01 -3.8674e-01 -1.9382e-01 -7.2564e-01 -4.6659e-02\n",
      " -1.0637e+00  2.8793e-01 -1.1768e-01 -2.7369e-01 -1.5231e-01  1.2132e-01\n",
      " -5.6115e-02 -3.8326e-02 -4.4486e-01  1.1362e-01  2.3695e-01 -4.8947e-01\n",
      " -2.2704e-01  2.1016e-01  1.4257e-01 -4.7739e-01  9.1464e-02 -1.1329e-01\n",
      " -7.0159e-01 -2.5456e-01  3.5839e-01 -9.3338e-02 -1.5415e-01  1.7727e-01\n",
      "  8.4677e-02 -3.6589e-01 -2.7441e-01 -4.3006e-01  4.0131e-01  1.6713e-01\n",
      "  2.5995e-01 -2.7959e-01 -1.0164e-01  4.8964e-01 -4.6979e-01  2.6124e-01\n",
      " -2.0056e-01 -2.4723e-01 -7.5513e-01  6.3123e-02  3.6439e-01  2.9707e-01\n",
      " -8.0736e-02 -4.2967e-01 -3.0026e-01 -3.2673e-02  1.9879e-01 -1.4637e-01\n",
      " -4.5976e-01 -2.0188e-01 -3.7192e-02  1.4920e-01  8.6076e-02  5.6872e-01\n",
      "  6.2187e-02  4.3064e-01 -1.6964e-01  5.3078e-01 -8.1827e-01 -4.1262e-02\n",
      "  2.5041e-01  1.0046e-01 -1.9745e-01  3.0148e-01 -8.1346e-02 -6.5166e-02\n",
      " -4.1397e-02  1.1750e-01  9.2175e-02  5.4948e-02 -3.3672e-03 -7.6881e-02\n",
      " -5.5310e-02  5.7904e-02  1.6996e-02 -2.3823e-01  1.5785e-01  2.4124e-01]\n"
     ]
    }
   ],
   "source": [
    "# word with meaning \n",
    "string_2 = 'health'\n",
    "embedding_2 = nlp('health')\n",
    "print(embedding_2[0].is_oov)\n",
    "print(embedding_2.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mindmap for the word **Health**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/mindmap.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------similarity scores----------\n",
      "diet to exercise : 0.582544108078496\n",
      "diet to food : 0.6076305053237465\n",
      "diet to vitamins : 0.6295147982810888\n",
      "diet to backyard : 0.1942055804386137\n"
     ]
    }
   ],
   "source": [
    "# similarity score between a selection of words\n",
    "token_1 = nlp('diet')\n",
    "words = 'exercise food vitamins backyard'\n",
    "tokens = nlp(words)\n",
    "print('------------similarity scores----------')\n",
    "for token in tokens:\n",
    "    print(token_1.text, 'to',token.text, ':', token_1.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the similarity score:\n",
    "cosine over vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dogs', 0.8835931), ('puppy', 0.85852146), ('pet', 0.8057451)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most similar words (closest vectors) \n",
    "def most_similar(word, topn=5):\n",
    "  word = nlp.vocab[str(word)]\n",
    "  queries = [\n",
    "      w for w in word.vocab \n",
    "      if w.is_lower == word.is_lower and w.prob >= -15 and np.count_nonzero(w.vector)\n",
    "  ]\n",
    "\n",
    "  by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "  return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]\n",
    "\n",
    "most_similar(\"dog\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('healthcare', 0.78880006),\n",
       " ('care', 0.74709594),\n",
       " ('wellness', 0.73053104),\n",
       " ('medical', 0.7086058),\n",
       " ('nutrition', 0.6945323)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diets', 0.89323026),\n",
       " ('dieting', 0.8071348),\n",
       " ('dietary', 0.7490052),\n",
       " ('calorie', 0.739019),\n",
       " ('foods', 0.73535097)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('diet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('influenza', 0.5106981),\n",
       " ('rabies', 0.45984036),\n",
       " ('virus', 0.44592503),\n",
       " ('hepatitis', 0.44005278),\n",
       " ('pathogen', 0.43760213),\n",
       " ('ebola', 0.43437657)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('coronavirus') \n",
    "# NB: embeddings are trained with news and media corpus from 2013 \n",
    "# the word coronavirus is already in the corpus, but not a common word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('influenza', 0.8240887),\n",
       " ('swine', 0.7921757),\n",
       " ('pandemic', 0.76574874),\n",
       " ('vaccine', 0.7106953),\n",
       " ('outbreak', 0.6994005)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('flu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> banana 0.5831845\n",
      "pasta <-> hippo 0.079349115\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "\n",
    "apple = doc[0]\n",
    "banana = doc[2]\n",
    "pasta = doc[6]\n",
    "hippo = doc[8]\n",
    "\n",
    "print(\"apple <-> banana\", apple.similarity(banana))\n",
    "print(\"pasta <-> hippo\", pasta.similarity(hippo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings visualized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/embeddings.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar words are close to each other in the vector space\n",
    "- using PCA to get 300 dimensions to 3 here for visualisation purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can use word vectors to find similar words and words used in the same context, <br> we can compare the meaning of single words, sentences or full documents, <br> and we can even perform simple math operations, working with the relationship between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# king - man + woman = queen  This describes a gender relationship.\n",
    "# Another example is: paris – france + germany = berlin. In this case, the vector difference between paris and france captures the concept of capital city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How are they trained? <a id='how'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**: two-layer neural networks that are trained to reconstruct linguistic contexts of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec can utilize either of two model architectures to produce a distributed representation of words: continuous bag-of-words (**CBOW**) or continuous **skip-gram**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train own model with gensim word2vec: easy to do and awesome build in features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How did we use word embeddings for beat classification? <a id='beat'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More information and recources <a id='more'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy: https://spacy.io/usage/spacy-101 <br> Spacy is ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim, word to vec ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StarSpace: https://github.com/facebookresearch/StarSpace <br> StarSpace is a general-purpose neural model for efficient learning of entity embeddings for solving a wide variety of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "米＾－＾米 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
